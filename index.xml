<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Lambda Continuum</title>
    <link>https://lambdacontinuum.github.io/</link>
    <description>Recent content in Home on Lambda Continuum</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Apr 2025 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://lambdacontinuum.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Trusting Our Future Selves</title>
      <link>https://lambdacontinuum.github.io/posts/trusting-future-selves/</link>
      <pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate>
      
      <guid>https://lambdacontinuum.github.io/posts/trusting-future-selves/</guid>
      <description>&lt;p&gt;On occasion I’ll be asked about some of the old articles I’ve written. It seems like some people really liked them. Sadly, I deleted all of them, so instead I’ll re-write a few of the ones I’ve been asked about and release them. This is one of those articles. I know my recent articles have been mostly old stuff so forgive me.&lt;/p&gt;
&lt;p&gt;Is it rational to believe what you would believe in the future? Consider the principle of reflection, where if one is rational then, letting Ct1 be their credence at time t1 and Ct2 be their credence at time t2, Ct1(H|Ct2(H) = x) = x.&lt;/p&gt;
&lt;p&gt;This is to say that if an agent is rational, then given that they know their later credence in some hypothesis H is x, the agent&amp;rsquo;s earlier credence in H should be x. One may understand this principle as expressing one&amp;rsquo;s trust in one&amp;rsquo;s future self such that they should defer to them.&lt;/p&gt;
&lt;p&gt;This may seem obvious to some, but it faces two types of counter-examples. The first is memory loss. Suppose t1 is 4/5/2025. Looking outside, I see the weather is rainy, and so my credence that the weather on 4/5/2025, is rainy is, let&amp;rsquo;s say, 0.999. I&amp;rsquo;ll suppose t2 is 4/5/2030, 5 years from now. If I was asked how likely it is the weather was rainy on 4/5/2025, I wouldn&amp;rsquo;t recall that it was rainy; maybe my credence would be around, I don&amp;rsquo;t know, 0.35? If I endorsed reflection, I should defer to my future self though, that is, Ct1(H|Ct2(H) = 0.35) = 0.35 but this seems clearly wrong. I should ignore my future forgetful state and hold Ct1(H|Ct2(H) = 0.35) = 0.999, so reflection cannot be a constraint on rationality. As long as one expects to forget in the future, they should violate reflection.&lt;/p&gt;
&lt;p&gt;David Christensen offers a hypothetical involving a drug called LSQ that, upon taking it, causes one to believe that they can fly by flapping their arms. So, after taking the drug at t2, Ct2(H) = 1, but knowing prior to taking the drug that you&amp;rsquo;ll be taking it at t1, you obviously shouldn&amp;rsquo;t believe you can fly, so Ct1(H|Ct2(H) = 1) = 0.001, which violates reflection. If one expects to be irrational in the future, they should violate reflection.&lt;/p&gt;
&lt;p&gt;So we have two instances of failure for reflection, but there does still feel something appealing about deferring to a version of ourselves with more experience, so maybe we should qualify reflection. Say in cases where we are sure we won&amp;rsquo;t forget anything and we are sure our belief change will be rational, i.e., all our belief change will be the result of conditionalization, we should satisfy reflection.&lt;/p&gt;
&lt;p&gt;Now it turns out that the qualification against memory loss is redundant as an agent whose beliefs are always the result of conditionalization won&amp;rsquo;t experience memory loss, so we can characterize our qualified reflection as saying agents should satisfy reflection if and only if they are certain they&amp;rsquo;ll always be conditionalizers.&lt;/p&gt;
&lt;p&gt;The problem is clear: we aren&amp;rsquo;t always conditionalizers. Quite the opposite, we&amp;rsquo;re flawed agents susceptible to forgetting and irrationality, and so we ought not abide by reflection. Ideally rational agents, on the other hand, ought to satisfy reflection. The upshot of all this is the constraints ideal agents ought to satisfy come apart from the constraints actual people in the epistemic landscape should satisfy. There are ways in which we ought not emulate ideal epistemic agents.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Divine Agency Without Failure</title>
      <link>https://lambdacontinuum.github.io/posts/divine-agency/</link>
      <pubDate>Mon, 27 Jan 2025 17:59:14 -0500</pubDate>
      
      <guid>https://lambdacontinuum.github.io/posts/divine-agency/</guid>
      <description>&lt;p&gt;Philosophy of Religion has recently seen a surge of popularity in philosophy circles online, so I’ve decided to throw my hat in the ring. Here’s an argument against God being an agent.&lt;/p&gt;
&lt;p&gt;Wittgenstein makes an interesting distinction. He distinguishes viewing intentions as mere mental processes from viewing intentions in terms of their fulfillment. For the latter, the idea here is that elements in the world can be taken to be in accord with such intentional states. Fulfillment conveys something that is normative in some minimal sense, leading us to think the very idea of intention is a normative idea. The general gist is that intentional states seem like the types of states that have standards by which one could be in accord or out of accord with them. Consider the cases of belief and desire. Suppose an agent believes that it’s raining outside, believes that an umbrella will protect them from the rain, and desires not to be wet. If such an agent were to walk outside without taking their umbrella, they’d be out of accord with their very own intentional states. The observation here is that intentional states seem to generate ideals of norm or correctness, and these norms are such that one could be in accord with or fail to accord with them. We can assess an agent’s actions as correct or incorrect by the light of the norms generated by their intentions. Thus, normativity is constitutive of intentional states.&lt;/p&gt;
&lt;p&gt;Furthermore, it seems uncontroversial that intentional states are constitutive of agency. If you were to conceive of an agent, such a being would have beliefs, desires, etc. But here’s the problem: If intentional states are constitutive of agency, and normativity is constitutive of intentional states, it follows that agency is constitutively normative. Why is this a problem? It’s said that the very possibility of failure is a necessary condition for normativity. That is, if you ought to do something, it follows there must be some possibility that you don’t do it.&lt;/p&gt;
&lt;p&gt;We established earlier that agency is a normative notion, and so agency must imply the possibility of failing to be in accord with one’s intentions—if it’s granted that a necessary condition for normativity is the possibility of failure. But traditional conceptions of God hold that His will is perfectly efficacious. God couldn’t fail to be in accord with His own intentional states. It follows that God couldn’t be an agent. The problem should be clear: God has been reduced to a matter of disposition, a mere causal tendency.&lt;/p&gt;
&lt;p&gt;What is to be done to defend God’s agency? One could deny that the possibility of failure is an essential component of normativity, or one could deny that the possibility of failure is an essential part of intentional states—thereby rejecting the claim that intentional states are normative. Here, I’ll take the latter approach. More specifically, I’ll undercut the considerations given for intentional states being normative.&lt;/p&gt;
&lt;p&gt;One may ask, as Akeel Bilgrami does, whether the general claim that intentional states are normative holds in the specific case of linguistic intentions—the intentions speakers have regarding the meaning of their words.&lt;/p&gt;
&lt;p&gt;The normativity in play when discussing meaning-intentions comes from the idea that we can act in accord with our intentions to use words. For example, uttering “That is a snake” to apply to snakes when we’re in the presence of snakes is acting in accord with our intention. This suggests that the intentions relevant to meaning target the truth-conditions of the words one uses.&lt;/p&gt;
&lt;p&gt;If we grant that the possibility of failure is a necessary condition for normativity, then for meaning-intentions to be normative, it must be possible to fail to get the truth-conditions of one’s utterances right. The most straightforward way to fail here would be if one doesn’t know the truth-conditions of the sentence they utter.&lt;/p&gt;
&lt;p&gt;But how could one intend their words to possess truth-conditions they don’t actually possess, simply because they don’t know those truth-conditions? This is the crucial question. Can one &lt;em&gt;not know&lt;/em&gt; the meanings of the words they intend to utter? The answer is that one cannot fail to know the truth-conditions of what they utter.&lt;/p&gt;
&lt;p&gt;Why? Recall that the idea that the meaning of a sentence is given by its truth-conditions was popularized by Frege’s doctrine of sense. Frege introduced sense to solve puzzles concerning informative identities. Suppose the meaning of a name consists solely in the object it picks out. Then two names that pick out the same object must have the same meaning. This creates a problem. Consider the names ‘Phosphorus’ and ‘Hesperus’, both of which pick out Venus:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Phosphorus = Phosphorus&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hesperus = Hesperus&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hesperus = Phosphorus&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can know &lt;em&gt;a priori&lt;/em&gt; that (1) and (2) are true, but (3) is not knowable &lt;em&gt;a priori&lt;/em&gt;. This reveals a key issue: Being uninformed is not the same as being irrational. If the meaning of a name consists solely in its referent, an agent would be irrational if they denied that Hesperus is Phosphorus. Yet such an agent isn’t irrational—they’re merely uninformed about the &lt;em&gt;a posteriori&lt;/em&gt; identity. The puzzle arises because a Millian theory of names conflates irrationality with ignorance. Frege’s solution posits “sense” (meaning) to preserve the distinction between logical error and empirical ignorance.&lt;/p&gt;
&lt;p&gt;Thus, any account of meaning must respect the following crucial desideratum: Agents in such puzzles should be seen as lacking empirical knowledge of the a posteriori identities at play, not as holding inconsistent mental states. Nothing should be tolerated in the understanding of the notion of meaning that prevents sense from doing what it was posited to do, solving the Fregean puzzles and maintaining the distinction between logical error and lack of empirical knowledge.&lt;/p&gt;
&lt;p&gt;Imagine someone wondering, “Is Hesperus Phosphorus?” This is intelligible if senses are posited. But if the wonderer doesn’t know their own senses, they might ask, “Is the sense of ‘Hesperus’ the same as the sense of ‘Phosphorus’?” The puzzle re-emerges! If meanings aren’t transparent to the speaker, Frege’s original problem resurfaces one level up, at the level of senses. This violates the basic desideratum that meaning must solve the puzzle it was posited to solve.&lt;/p&gt;
&lt;p&gt;And so, one cannot fail to know the meaning of the sentences they intend to utter. This follows from the requirement that meaning must resolve Fregean identity puzzles decisively—a fundamentally Fregean argument for the transparency of meaning. This argument blocks the possibility of failing to act in accord with meaning-intentions due to ignorance of the truth-conditions. There are other sources that may contribute to the possibility of failing to be in accord with the intentions to say something with certain truth conditions. Another major one is misspeaking. I won’t address this here to avoid making the article overly long. If you’re interested, check out &lt;em&gt;Why Meaning Intentions Are Degenerate&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Meaning-intentions thus serve as an exception to Wittgenstein’s insight about the normativity of intentions. Bilgrami grants that Wittgenstein is generally correct about intentions being normative. However, what I gather is that intentions need not &lt;em&gt;necessarily&lt;/em&gt; be normative. The reasoning for intentional states being normative hinges on the idea that intentions can be in or out of accord with actions. But such reasoning doesn’t guarantee all intentions are intrinsically normative. Meaning-intentions act as a counterexample: They’re a degenerate case. Then it wouldn’t follow that normativity is constitutive of agency. Meaning being an agent doesn’t necessarily imply the possibility of failure.&lt;/p&gt;
&lt;p&gt;To be clear, what I’m providing is an undercutting argument. The argument given for intentions being normative fails at guaranteeing that normativity is constitutive of intentionality, and so agency need not imply the possibility of failure. God’s agency—as a being whose intentions cannot fail—remains preserved.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contra Standard Deontic Logic</title>
      <link>https://lambdacontinuum.github.io/posts/contra-sdl/</link>
      <pubDate>Thu, 23 Jan 2025 17:57:25 -0500</pubDate>
      
      <guid>https://lambdacontinuum.github.io/posts/contra-sdl/</guid>
      <description>&lt;p&gt;Since I’ve started publishing again, I decided to begin with a re-write of an old article titled something alon&lt;/p&gt;
&lt;p&gt;Bengt Hansson introduced the term &lt;em&gt;Standard Deontic Logic&lt;/em&gt; (referred to as SDL from here on) to denote deontic&lt;/p&gt;
&lt;p&gt;ℐ represents the set of deontically perfect worlds. Sentences valid in such a model coincide with the sentences&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Op → ~O~p,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Op ∧ Oq ↔ O(p ∧ q),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;O(p ∨ q).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The second axiom is equivalent to the combination of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Op ∧ Oq → O(p ∧ q), i.e. agglomeration.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If Op and p logically implies q, then Oq, i.e. necessitation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now that we’ve characterized SDL, why do I think it fails to properly model obligation? Consider the following&lt;/p&gt;
&lt;p&gt;First is the problem of free choice permission. Imagine your neighbor asked to borrow a crowbar. You showed him&lt;/p&gt;
&lt;p&gt;Such a principle doesn’t hold in SDL. An obvious solution might be to simply add the principle to the list of S&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;O(~a ∧ ~b) → O~a, Holds in SDL.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;O~(a ∨ b) → O~a, Equiv to 1.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;~O~a → ~O~(a ∨ b), Contrapositive.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pa → P(a ∨ b), Definition of P.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pa → Pb, From added postulate.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The axiom combined with the original postulates entails that if something is permitted, then everything else is permitted. This is intolerable, so we cannot solve the issue of free choice by turning the SDL permission operator into an operator of free choice permission.&lt;/p&gt;
&lt;p&gt;Some deontic logicians attempt to fix this by introducing a second permission operator Pc into SDL. One such definition is Pc(a ∨ b) ↔ Pa ∧ Pb. This definition is implausible, see the following derivation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pa → (Pa ∧ P(a ∨ b)), Pa → P(a ∨ b) holds in SDL.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pa → Pc(a ∨ (a ∨ b)), Definition of Pc&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pa → Pc(a ∨ b).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, if you’re permitted to borrow a screwdriver, it follows that you have free choice to either borrow or steal the screwdriver. Other attempts to construct free choice operators have similarly absurd consequences. This is because such constructions rely on the &lt;em&gt;single sentence assumption&lt;/em&gt; —the idea that free choice between a and b can be represented as a property of a single sentence, namely a∨b.&lt;/p&gt;
&lt;p&gt;Under this assumption, logically equivalent sentences are interchangeable. Thus, if a ∨ b is equivalent to c ∨ d, then there is a free choice permission between a and b if and only if there is a free choice permission between c and d. This entails implausible results. Consider the case of the vegetarian’s free lunch. In a restaurant, I may have a meal with meat or a meal without meat. Therefore, I may either have a meal and pay for it or have a meal and not pay for it. This follows because: Let m denote having a meal with meat, v a meal without meat, and p paying. P(m ∨ v) is equivalent to P(((m ∨ v) ∧ p) ∨ ((m ∨ v) ∧ ~p)).&lt;/p&gt;
&lt;p&gt;Free choice permission illustrates an intuitive deontic principle that doesn’t hold in SDL. Conversely, SDL sometimes validates principles that are not plausible. We call such cases deontic paradoxes. Consider Åqvist&amp;rsquo;s knower paradox: If a police officer ought to know that Smith robbed Jones, then Smith ought to rob Jones. This counterexample challenges the principle of necessitation. Von Wright correctly points out that all major deontic paradoxes rely on necessitation. However, necessitation follows from SDL’s semantic construction of possible worlds, so these paradoxes challenge SDL’s semantics itself.&lt;/p&gt;
&lt;p&gt;SDL also fails to model certain important kinds of obligations commonly discussed—namely, obligations of compensation, reparation, and prevention. Recall that under SDL’s semantics, obligatory acts in the actual world consist of actions that would occur in ideal worlds. Suppose there’s a drowning child in front of you. In a deontically ideal world, no child would drown, so you wouldn’t save a child from drowning in an ideal world because no drowning child would exist. It follows that you are not obligated to save the drowning child.&lt;/p&gt;
&lt;p&gt;Obligations of compensation, reparation, and prevention require the existence of actions that need be compensated, reparated, and prevented. But these are actions that are unideal and wouldn’t occur in an ideal world. Consequently, SDL’s semantics cannot model such obligations. This is a significant problem, as such obligations are central not only to everyday moral discourse but also to moral philosophy as a whole. As Holly Goldman puts it, SDL &amp;ldquo;ignores the fact that particular obligations flow from abstract principles together with contingent features of the world,&amp;rdquo; and such features &amp;ldquo;do not appear in all the morally best worlds.&amp;rdquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About Me</title>
      <link>https://lambdacontinuum.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lambdacontinuum.github.io/about/</guid>
      <description>&lt;h2 id=&#34;links&#34;&gt;Links:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Twitter:&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/w0arna&#34;&gt;@w0arna&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Substack:&lt;/strong&gt; &lt;a href=&#34;https://minimalfixedpoint.substack.com/&#34;&gt;Minimal Fixed Point&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=&#34;https://github.com/LambdaContinuum&#34;&gt;LambdaContinuum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;software-i-use&#34;&gt;Software I use:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; NixOS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Window Manager:&lt;/strong&gt; Hyprland&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Terminal:&lt;/strong&gt; Kitty&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Text Editor:&lt;/strong&gt; NeoVim/NVF configuration&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Posts</title>
      <link>https://lambdacontinuum.github.io/posts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://lambdacontinuum.github.io/posts/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
